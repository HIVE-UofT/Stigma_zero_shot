{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8d69d2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-07T12:02:22.300724Z",
     "iopub.status.busy": "2025-02-07T12:02:22.300180Z",
     "iopub.status.idle": "2025-02-07T12:02:32.017490Z",
     "shell.execute_reply": "2025-02-07T12:02:32.015797Z"
    },
    "papermill": {
     "duration": 9.726479,
     "end_time": "2025-02-07T12:02:32.019944",
     "exception": false,
     "start_time": "2025-02-07T12:02:22.293465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q openai\n",
    "! pip install -q gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93293b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:02:32.030379Z",
     "iopub.status.busy": "2025-02-07T12:02:32.029999Z",
     "iopub.status.idle": "2025-02-07T12:02:33.698699Z",
     "shell.execute_reply": "2025-02-07T12:02:33.697367Z"
    },
    "papermill": {
     "duration": 1.676101,
     "end_time": "2025-02-07T12:02:33.700696",
     "exception": false,
     "start_time": "2025-02-07T12:02:32.024595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d852a9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:02:33.710060Z",
     "iopub.status.busy": "2025-02-07T12:02:33.709446Z",
     "iopub.status.idle": "2025-02-07T12:03:24.363384Z",
     "shell.execute_reply": "2025-02-07T12:03:24.361949Z"
    },
    "papermill": {
     "duration": 50.66105,
     "end_time": "2025-02-07T12:03:24.365753",
     "exception": false,
     "start_time": "2025-02-07T12:02:33.704703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1rA0EtiqvFmyxulPIrtOGrEynGYki10Oc\r\n",
      "To: /kaggle/working/reddit_data.json\r\n",
      "100%|██████████████████████████████████████| 6.55M/6.55M [00:00<00:00, 29.5MB/s]\r\n",
      "Downloading...\r\n",
      "From (original): https://drive.google.com/uc?id=1-jF6BRzrXySV1XrU5-SPOUammX9QA-kw\r\n",
      "From (redirected): https://drive.google.com/uc?id=1-jF6BRzrXySV1XrU5-SPOUammX9QA-kw&confirm=t&uuid=fe89e006-a307-4657-ab7c-e4813549103a\r\n",
      "To: /kaggle/working/HealthUnlocked_crawled.json\r\n",
      "100%|████████████████████████████████████████| 195M/195M [00:02<00:00, 76.3MB/s]\r\n",
      "Downloading...\r\n",
      "From (original): https://drive.google.com/uc?id=1-50BWoOq3QTcsbI2zh8D89GEt3TVADmA\r\n",
      "From (redirected): https://drive.google.com/uc?id=1-50BWoOq3QTcsbI2zh8D89GEt3TVADmA&confirm=t&uuid=439a1e5c-e048-4eca-99df-cc1bcedfaa4f\r\n",
      "To: /kaggle/working/TU_crawled.json\r\n",
      "100%|████████████████████████████████████████| 121M/121M [00:02<00:00, 60.1MB/s]\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1ECHFIP6W-e4fCwIfrBlRkrec6vyBlJdN\r\n",
      "To: /kaggle/working/reddit_t1_data.json\r\n",
      "100%|███████████████████████████████████████| 3.32M/3.32M [00:00<00:00, 168MB/s]\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=174NFCa6IlkSjoRCItRcvQxdjZFd1XGlT\r\n",
      "To: /kaggle/working/reddit_t2_data.json\r\n",
      "100%|███████████████████████████████████████| 4.03M/4.03M [00:00<00:00, 186MB/s]\r\n"
     ]
    }
   ],
   "source": [
    "# reddit\n",
    "! gdown 1rA0EtiqvFmyxulPIrtOGrEynGYki10Oc\n",
    "\n",
    "# healthunlocked\n",
    "! gdown 1-jF6BRzrXySV1XrU5-SPOUammX9QA-kw\n",
    "\n",
    "# tudiabetes\n",
    "! gdown 1-50BWoOq3QTcsbI2zh8D89GEt3TVADmA\n",
    "\n",
    "# reddit-t1\n",
    "! gdown 1ECHFIP6W-e4fCwIfrBlRkrec6vyBlJdN\n",
    "\n",
    "# reddit-t2\n",
    "! gdown 174NFCa6IlkSjoRCItRcvQxdjZFd1XGlT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d7c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:03:24.380326Z",
     "iopub.status.busy": "2025-02-07T12:03:24.379966Z",
     "iopub.status.idle": "2025-02-07T12:03:25.021660Z",
     "shell.execute_reply": "2025-02-07T12:03:25.020654Z"
    },
    "papermill": {
     "duration": 0.651445,
     "end_time": "2025-02-07T12:03:25.023798",
     "exception": false,
     "start_time": "2025-02-07T12:03:24.372353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=)\n",
    "models = client.models.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f5521",
   "metadata": {
    "papermill": {
     "duration": 0.005343,
     "end_time": "2025-02-07T12:03:25.035210",
     "exception": false,
     "start_time": "2025-02-07T12:03:25.029867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386306f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T12:03:25.048369Z",
     "iopub.status.busy": "2025-02-07T12:03:25.047859Z",
     "iopub.status.idle": "2025-02-07T13:08:10.685523Z",
     "shell.execute_reply": "2025-02-07T13:08:10.684064Z"
    },
    "papermill": {
     "duration": 3885.646385,
     "end_time": "2025-02-07T13:08:10.687497",
     "exception": false,
     "start_time": "2025-02-07T12:03:25.041112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:04:45<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/working/reddit_data.json', 'r') as f:\n",
    "  reddit_data = json.load(f)\n",
    "  f.close()\n",
    "\n",
    "reddit_answers = []\n",
    "\n",
    "for i in trange(5000):\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are an advanced text analysis AI designed to identify and label social media content for any signs of stigmatized language or messaging regarding people with diabetes. You have expertise in understanding bias, stereotypes, and stigmatizing terms, and you apply your knowledge impartially. When presented with a social media post, you will classify the content based on whether it contains: 1. **Stigmatized Content**: Language that is discriminatory, biased, shaming, or marginalizing towards individuals with diabetes. 2. **Neutral Content**: Language that is neutral, factual, or supportive without stigmatizing. For each post, you must provide: 1. **Label**: 'Stigmatized Content' or 'Neutral Content.' 2. **Explanation**: A brief rationale for why the post was labeled this way. Your decisions must be based on the specific words and tone of the post, avoiding personal interpretation or assumptions about the author's intent unless explicitly stated in the text.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"Here is a social media post. Please classify it as either 'Stigmatized Content' or 'Neutral Content' based on its language and tone regarding people with diabetes. Provide a clear explanation for your classification. **Post title**: {reddit_data[i]['title']}, **Post main body**: {reddit_data[i]['text']}\"\n",
    "          }\n",
    "      ],\n",
    "      max_tokens = 10,\n",
    "      temperature = 0.0\n",
    "  )\n",
    "\n",
    "  if i % 100 == 0:\n",
    "      with open('reddit_answers.json', 'w') as f:\n",
    "          f.write(json.dumps(reddit_answers))\n",
    "\n",
    "  reddit_answers.append(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e93b250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T13:08:11.252708Z",
     "iopub.status.busy": "2025-02-07T13:08:11.252373Z",
     "iopub.status.idle": "2025-02-07T13:08:11.259219Z",
     "shell.execute_reply": "2025-02-07T13:08:11.258292Z"
    },
    "papermill": {
     "duration": 0.287621,
     "end_time": "2025-02-07T13:08:11.260973",
     "exception": false,
     "start_time": "2025-02-07T13:08:10.973352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('reddit_total_answers.json', 'w') as f:\n",
    "    f.write(json.dumps(reddit_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235901c",
   "metadata": {
    "papermill": {
     "duration": 0.288568,
     "end_time": "2025-02-07T13:08:11.948389",
     "exception": false,
     "start_time": "2025-02-07T13:08:11.659821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# HealthUnlocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024314c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T13:08:12.510245Z",
     "iopub.status.busy": "2025-02-07T13:08:12.509829Z",
     "iopub.status.idle": "2025-02-07T13:12:07.426624Z",
     "shell.execute_reply": "2025-02-07T13:12:07.424624Z"
    },
    "papermill": {
     "duration": 235.196845,
     "end_time": "2025-02-07T13:12:07.428358",
     "exception": true,
     "start_time": "2025-02-07T13:08:12.231513",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 312/5000 [03:53<58:23,  1.34it/s]  \n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a7fd360d55a5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       messages=[\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    827\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1278\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         )\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m     def patch(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    958\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1096\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/working/HealthUnlocked_crawled.json', 'r') as f:\n",
    "  healthunlocked_data = json.load(f)\n",
    "  f.close()\n",
    "\n",
    "\n",
    "healthunlocked_answers = []\n",
    "\n",
    "# len(healthunlocked_data)\n",
    "for i in trange(5000):\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are an advanced text analysis AI designed to identify and label social media content for any signs of stigmatized language or messaging regarding people with diabetes. You have expertise in understanding bias, stereotypes, and stigmatizing terms, and you apply your knowledge impartially. When presented with a social media post, you will classify the content based on whether it contains: 1. **Stigmatized Content**: Language that is discriminatory, biased, shaming, or marginalizing towards individuals with diabetes. 2. **Neutral Content**: Language that is neutral, factual, or supportive without stigmatizing. For each post, you must provide: 1. **Label**: 'Stigmatized Content' or 'Neutral Content.' 2. **Explanation**: A brief rationale for why the post was labeled this way. Your decisions must be based on the specific words and tone of the post, avoiding personal interpretation or assumptions about the author's intent unless explicitly stated in the text.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"Here is a social media post. Please classify it as either 'Stigmatized Content' or 'Neutral Content' based on its language and tone regarding people with diabetes. Provide an explanation for your classification. **Post title**: {healthunlocked_data[i]['title']}, **All chat messages**: {healthunlocked_data[i]['post']}\"\n",
    "          }\n",
    "      ],\n",
    "      max_tokens = 10,\n",
    "      temperature = 0.0\n",
    "\n",
    "  )\n",
    "\n",
    "  if i % 100 == 0:\n",
    "      with open('healthunlocked_answers.json', 'w') as f:\n",
    "          f.write(json.dumps(healthunlocked_answers))\n",
    "          f.close()\n",
    "\n",
    "  healthunlocked_answers.append(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368bf17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T11:33:11.173773Z",
     "iopub.status.busy": "2025-02-05T11:33:11.173362Z",
     "iopub.status.idle": "2025-02-05T11:33:11.179060Z",
     "shell.execute_reply": "2025-02-05T11:33:11.177995Z",
     "shell.execute_reply.started": "2025-02-05T11:33:11.173736Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('healthunlocked_total_answers.json', 'w') as f:\n",
    "    f.write(json.dumps(healthunlocked_answers))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba31c6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# TUDiabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a43046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T11:36:15.188693Z",
     "iopub.status.busy": "2025-02-05T11:36:15.188353Z",
     "iopub.status.idle": "2025-02-05T11:36:39.324755Z",
     "shell.execute_reply": "2025-02-05T11:36:39.323881Z",
     "shell.execute_reply.started": "2025-02-05T11:36:15.188669Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/working/TU_crawled.json', 'r') as f:\n",
    "  tudiabetes_data = json.load(f)\n",
    "  f.close()\n",
    "\n",
    "tudiabetes_answers = []\n",
    "\n",
    "# len(tudiabetes_data)\n",
    "for i in trange(5000):\n",
    "  all_messages = '\\n'.join(tudiabetes_data[i]['chat_list'])\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are an advanced text analysis AI designed to identify and label social media content for any signs of stigmatized language or messaging regarding people with diabetes. You have expertise in understanding bias, stereotypes, and stigmatizing terms, and you apply your knowledge impartially. When presented with a social media post, you will classify the content based on whether it contains: 1. **Stigmatized Content**: Language that is discriminatory, biased, shaming, or marginalizing towards individuals with diabetes. 2. **Neutral Content**: Language that is neutral, factual, or supportive without stigmatizing. For each post, you must provide: 1. **Label**: 'Stigmatized Content' or 'Neutral Content.' 2. **Explanation**: A brief rationale for why the post was labeled this way. Your decisions must be based on the specific words and tone of the post, avoiding personal interpretation or assumptions about the author's intent unless explicitly stated in the text.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"Here is a social media post. Please classify it as either 'Stigmatized Content' or 'Neutral Content' based on its language and tone regarding people with diabetes. Provide an explanation for your classification. **Post title**: {tudiabetes_data[i]['title']}, **All chat messages**: {all_messages}\"\n",
    "          }\n",
    "      ],\n",
    "      max_tokens = 10,\n",
    "      temperature = 0.0\n",
    "\n",
    "  )\n",
    "\n",
    "  if i % 100 == 0:\n",
    "      with open('tudiabetes_answers.json', 'w') as f:\n",
    "          f.write(json.dumps(tudiabetes_answers))\n",
    "          f.close()\n",
    "\n",
    "  tudiabetes_answers.append(completion.choices[0].message.content)\n",
    "\n",
    "\n",
    "with open('tudiabetes_total_answers.json', 'w') as f:\n",
    "    f.write(json.dumps(tudiabetes_answers))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5b91f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4189.469009,
   "end_time": "2025-02-07T13:12:08.751769",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-07T12:02:19.282760",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
