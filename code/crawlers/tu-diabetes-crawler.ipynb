{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6015ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-18T17:43:01.036694Z",
     "iopub.status.busy": "2025-01-18T17:43:01.036288Z",
     "iopub.status.idle": "2025-01-18T17:43:01.367875Z",
     "shell.execute_reply": "2025-01-18T17:43:01.366836Z"
    },
    "papermill": {
     "duration": 0.337638,
     "end_time": "2025-01-18T17:43:01.370009",
     "exception": false,
     "start_time": "2025-01-18T17:43:01.032371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from threading import Lock\n",
    "import json\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24048dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:43:01.380587Z",
     "iopub.status.busy": "2025-01-18T17:43:01.380011Z",
     "iopub.status.idle": "2025-01-18T17:43:01.408523Z",
     "shell.execute_reply": "2025-01-18T17:43:01.407296Z"
    },
    "papermill": {
     "duration": 0.036182,
     "end_time": "2025-01-18T17:43:01.410647",
     "exception": false,
     "start_time": "2025-01-18T17:43:01.374465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    \"\"\"\n",
    "    put your own user agent in the headers\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US'\n",
    "    }\n",
    "    top_URL = 'https://forum.tudiabetes.org/'\n",
    "\n",
    "    def __init__(self, crawling_threshold=1000):\n",
    "        \"\"\"\n",
    "        Initialize the crawler\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        crawling_threshold: int\n",
    "            The number of pages to crawl\n",
    "        \"\"\"\n",
    "        \n",
    "        self.crawling_threshold = crawling_threshold\n",
    "        self.not_crawled = []\n",
    "        self.crawled = []\n",
    "        self.crawled_ids = []\n",
    "        self.added_ids = []\n",
    "        self.add_list_lock = Lock()\n",
    "        self.add_queue_lock = Lock()\n",
    "\n",
    "    def get_id_from_URL(self, URL):\n",
    "        \"\"\"\n",
    "        Get the id from the URL of the site. The id is what comes exactly after title.\n",
    "        for example the id for the movie https://www.imdb.com/title/tt0111161/?ref_=chttp_t_1 is tt0111161.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        Returns\n",
    "        ----------\n",
    "        str\n",
    "            The id of the site\n",
    "        \"\"\"\n",
    "        \n",
    "        return '/'.join(URL.split('/')[3:])\n",
    "\n",
    "    def write_to_file_as_json(self):\n",
    "        \"\"\"\n",
    "        Save the crawled files into json\n",
    "        \"\"\"\n",
    "        with open('TU_crawled.json', 'w') as f:\n",
    "            f.write(json.dumps(self.crawled))\n",
    "            f.close()\n",
    "\n",
    "        with open('TU_not_crawled.json', 'w') as f:\n",
    "            f.write(json.dumps(self.not_crawled))\n",
    "            f.close()\n",
    "\n",
    "    def read_from_file_as_json(self):\n",
    "        \"\"\"\n",
    "        Read the crawled files from json\n",
    "        \"\"\"\n",
    "        \n",
    "        with open('TU_crawled.json', 'r') as f:\n",
    "            raw_data_0 = f.read()\n",
    "            f.close()\n",
    "\n",
    "        self.crawled = json.loads(raw_data_0)\n",
    "\n",
    "        with open('TU_not_crawled.json', 'w') as f:\n",
    "            raw_data_1 = f.read()\n",
    "            f.close()\n",
    "\n",
    "        self.not_crawled = json.loads(raw_data_1)\n",
    "\n",
    "        self.added_ids = [self.get_id_from_URL(link) for link in self.not_crawled]\n",
    "\n",
    "    def crawl(self, URL):\n",
    "        \"\"\"\n",
    "        Make a get request to the URL and return the response\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        Returns\n",
    "        ----------\n",
    "        requests.models.Response\n",
    "            The response of the get request\n",
    "        \"\"\"\n",
    "        user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "        ]\n",
    "        Crawler.headers['User-Agent'] = random.choice(user_agents)\n",
    "        res = get(URL, headers=Crawler.headers)\n",
    "        if res.status_code == 200:\n",
    "            return res\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_top_links(self):\n",
    "        \"\"\"\n",
    "        Extract the top 250 movies from the top 250 page and use them as seed for the crawler to start crawling.\n",
    "        \"\"\"\n",
    "        \n",
    "        res = self.crawl(self.top_URL)\n",
    "        top_soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        link_elements = top_soup.select('a[href]')\n",
    "        abs = 'https://forum.tudiabetes.org/'\n",
    "        for link in link_elements:\n",
    "            if link['href'].startswith('https://forum.tudiabetes.org/t/'):\n",
    "                id = self.get_id_from_URL(link['href'])\n",
    "                new_url = abs + id\n",
    "                if new_url not in self.not_crawled:\n",
    "                    self.not_crawled.append(new_url)\n",
    "                    self.added_ids.append(id)\n",
    "\n",
    "\n",
    "    def get_page_instance(self):\n",
    "        return {\n",
    "            'id': None,\n",
    "            'title': None,\n",
    "            'category': None,\n",
    "            'chat_list': None,\n",
    "            'related_links': None\n",
    "        }\n",
    "\n",
    "    def start_crawling(self):\n",
    "        \"\"\"\n",
    "        Start crawling the movies until the crawling threshold is reached.\n",
    "    \n",
    "        ThreadPoolExecutor is used to make the crawler faster by using multiple threads to crawl the pages.\n",
    "        You are free to use it or not. If used, not to forget safe access to the shared resources.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "        self.extract_top_links()\n",
    "        futures = []\n",
    "        crawled_counter = 0\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            while len(self.crawled) < self.crawling_threshold:\n",
    "\n",
    "                self.add_list_lock.acquire()\n",
    "                URL = self.not_crawled.pop(0)\n",
    "                self.add_list_lock.release()\n",
    "                futures.append(executor.submit(self.crawl_page_info, URL))\n",
    "                if len(self.not_crawled) == 0:\n",
    "                    wait(futures)\n",
    "                    futures = []\n",
    "\n",
    "    def crawl_page_info(self, URL):\n",
    "        \"\"\"\n",
    "        Main Logic of the crawler. It crawls the page and extracts the information of the movie.\n",
    "        Use related links of a movie to crawl more movies.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        if self.get_id_from_URL(URL) in self.crawled_ids or len(self.crawled) >= self.crawling_threshold:\n",
    "          return\n",
    "\n",
    "        page_id = self.get_id_from_URL(URL)\n",
    "\n",
    "        res = self.crawl(URL)\n",
    "        time.sleep(10)\n",
    "\n",
    "        if (res is not None):\n",
    "            page_instance = self.get_page_instance()\n",
    "            self.extract_movie_info(res, page_instance, URL)\n",
    "\n",
    "            self.add_list_lock.acquire()\n",
    "            self.crawled.append(page_instance)\n",
    "            self.added_ids.remove(page_id)\n",
    "            self.crawled_ids.append(page_id)\n",
    "\n",
    "            for link in page_instance['related_links']:\n",
    "                self.not_crawled.append(link)\n",
    "                self.added_ids.append(self.get_id_from_URL(link))\n",
    "\n",
    "            self.add_list_lock.release()\n",
    "\n",
    "            return page_instance\n",
    "        else:\n",
    "\n",
    "            self.add_list_lock.acquire()\n",
    "            self.not_crawled.append(URL)\n",
    "            self.add_list_lock.release()\n",
    "\n",
    "    def extract_movie_info(self, res, page, URL):\n",
    "        \"\"\"\n",
    "        Extract the information of the movie from the response and save it in the movie instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        res: requests.models.Response\n",
    "            The response of the get request\n",
    "        movie: dict\n",
    "            The instance of the movie\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "\n",
    "        main_soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        page['id'] = self.get_id_from_URL(URL)\n",
    "        page['title'] = URL.split('/')[-2]\n",
    "        page['category'] = self.get_category(main_soup)\n",
    "        page['chat_list'] = self.get_chatlist(main_soup)\n",
    "        page['related_links'] = self.get_related_links(main_soup)\n",
    "\n",
    "\n",
    "    def get_chatlist(self, soup):\n",
    "      chat_elements = soup.find_all(class_ = 'post')\n",
    "      chats = []\n",
    "      for chat in chat_elements:\n",
    "        s = chat.find_all('p')\n",
    "        text = ''\n",
    "        for d in s:\n",
    "          text = text +  d.text + '\\n'\n",
    "        chats.append(text)\n",
    "      return chats\n",
    "\n",
    "    def get_category(self, soup):\n",
    "      spans = soup.find_all('span', class_ = 'category-name')\n",
    "      return spans[0].get_text()\n",
    "\n",
    "    def get_related_links(self, soup):\n",
    "        \"\"\"\n",
    "        Get the related links of the movie from the More like this section of the page from the soup\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        soup: BeautifulSoup\n",
    "            The soup of the page\n",
    "        Returns\n",
    "        ----------\n",
    "        List[str]\n",
    "            The related links of the movie\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        link_elements = soup.select('a[href]')\n",
    "        for link in link_elements:\n",
    "          if link['href'].startswith('https://forum.tudiabetes.org/t'):\n",
    "            links.append(link['href'])\n",
    "        return links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa4907c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T17:43:01.418549Z",
     "iopub.status.busy": "2025-01-18T17:43:01.418103Z",
     "iopub.status.idle": "2025-01-18T20:51:03.014616Z",
     "shell.execute_reply": "2025-01-18T20:51:03.013136Z"
    },
    "papermill": {
     "duration": 11281.603298,
     "end_time": "2025-01-18T20:51:03.017257",
     "exception": false,
     "start_time": "2025-01-18T17:43:01.413959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "crawler = Crawler(crawling_threshold=20000)\n",
    "crawler.start_crawling()\n",
    "crawler.write_to_file_as_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3521d",
   "metadata": {
    "papermill": {
     "duration": 0.00188,
     "end_time": "2025-01-18T20:51:03.022198",
     "exception": false,
     "start_time": "2025-01-18T20:51:03.020318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11285.91087,
   "end_time": "2025-01-18T20:51:03.961688",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-18T17:42:58.050818",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
