{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9856a69f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-04T08:24:20.728266Z",
     "iopub.status.busy": "2025-01-04T08:24:20.727922Z",
     "iopub.status.idle": "2025-01-04T08:24:21.035539Z",
     "shell.execute_reply": "2025-01-04T08:24:21.034520Z"
    },
    "papermill": {
     "duration": 0.313266,
     "end_time": "2025-01-04T08:24:21.037393",
     "exception": false,
     "start_time": "2025-01-04T08:24:20.724127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from requests import Session ,get\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from threading import Lock\n",
    "import json\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851b92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T08:24:21.043860Z",
     "iopub.status.busy": "2025-01-04T08:24:21.043286Z",
     "iopub.status.idle": "2025-01-04T08:24:21.070580Z",
     "shell.execute_reply": "2025-01-04T08:24:21.069538Z"
    },
    "papermill": {
     "duration": 0.03222,
     "end_time": "2025-01-04T08:24:21.072353",
     "exception": false,
     "start_time": "2025-01-04T08:24:21.040133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    \"\"\"\n",
    "    put your own user agent in the headers\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US'\n",
    "    }\n",
    "    top_URL = 'https://healthunlocked.com/'\n",
    "\n",
    "    def __init__(self, crawling_threshold=1000):\n",
    "        \"\"\"\n",
    "        Initialize the crawler\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        crawling_threshold: int\n",
    "            The number of pages to crawl\n",
    "        \"\"\"\n",
    "        \n",
    "        self.crawling_threshold = crawling_threshold\n",
    "        self.session = None\n",
    "        self.not_crawled = []\n",
    "        self.crawled = []\n",
    "        self.crawled_ids = []\n",
    "        self.added_ids = []\n",
    "        self.add_list_lock = Lock()\n",
    "        self.add_queue_lock = Lock()\n",
    "\n",
    "    def get_id_from_URL(self, URL):\n",
    "        \"\"\"\n",
    "        Get the id from the URL of the site. The id is what comes exactly after title.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        Returns\n",
    "        ----------\n",
    "        str\n",
    "            The id of the site\n",
    "        \"\"\"\n",
    "        \n",
    "        return '/'.join(URL.split('/')[3:])\n",
    "\n",
    "    def write_to_file_as_json(self):\n",
    "        \"\"\"\n",
    "        Save the crawled files into json\n",
    "        \"\"\"\n",
    "        \n",
    "        with open('HealthUnlocked_crawled.json', 'w') as f:\n",
    "            f.write(json.dumps(self.crawled))\n",
    "            f.close()\n",
    "\n",
    "        with open('HealthUnlocked_not_crawled.json', 'w') as f:\n",
    "            f.write(json.dumps(self.not_crawled))\n",
    "            f.close()\n",
    "\n",
    "    def read_from_file_as_json(self):\n",
    "        \"\"\"\n",
    "        Read the crawled files from json\n",
    "        \"\"\"\n",
    "        \n",
    "        with open('HealthUnlocked_crawled.json', 'r') as f:\n",
    "            raw_data_0 = f.read()\n",
    "            f.close()\n",
    "\n",
    "        self.crawled = json.loads(raw_data_0)\n",
    "\n",
    "        with open('HealthUnlocked_not_crawled.json', 'w') as f:\n",
    "            raw_data_1 = f.read()\n",
    "            f.close()\n",
    "\n",
    "        self.not_crawled = json.loads(raw_data_1)\n",
    "\n",
    "        self.added_ids = [self.get_id_from_URL(link) for link in self.not_crawled]\n",
    "\n",
    "    def login(self):\n",
    "      self.session = Session()\n",
    "\n",
    "      login_url = 'https://healthunlocked.com/login'\n",
    "\n",
    "      credentials = {\n",
    "          'username': 'amirhoseinrezayi95@gmail.com',\n",
    "          'password': 'H@ji1380'\n",
    "      }\n",
    "\n",
    "      response = self.session.post(login_url, data=credentials)\n",
    "      if response.ok:\n",
    "          pass\n",
    "      else:\n",
    "          raise Exception('Login failed')\n",
    "\n",
    "    def crawl(self, URL):\n",
    "        \"\"\"\n",
    "        Make a get request to the URL and return the response\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        Returns\n",
    "        ----------\n",
    "        requests.models.Response\n",
    "            The response of the get request\n",
    "        \"\"\"\n",
    "        user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
    "        ]\n",
    "        Crawler.headers['User-Agent'] = random.choice(user_agents)\n",
    "        res = self.session.get(URL, headers=Crawler.headers)\n",
    "        if res.status_code == 200:\n",
    "            return res\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_top_links(self):\n",
    "        \"\"\"\n",
    "        Extract the top 250 movies from the top 250 page and use them as seed for the crawler to start crawling.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        for i in range(1, 11):\n",
    "          top_URL = f'https://healthunlocked.com/tag/diabetes?page={i}'\n",
    "          res = self.crawl(top_URL)\n",
    "          top_soup = BeautifulSoup(res.content, 'html.parser')\n",
    "          links = self.extract_source_links(top_soup)\n",
    "          for link in links:\n",
    "            if link not in self.not_crawled:\n",
    "              self.not_crawled.append(link)\n",
    "              self.added_ids.append(self.get_id_from_URL(link))\n",
    "\n",
    "    def extract_source_links(self, soup):\n",
    "      extracted_links = []\n",
    "      A = soup.find_all('div', class_ = 'sc-6d595cea-6 ecnMcz')\n",
    "      for x in A:\n",
    "        a = x.find_all('a')\n",
    "        for b in a:\n",
    "          if 'posts' in b['href'] and '?responses' not in b['href']:\n",
    "            extracted_links.append('https://healthunlocked.com' + b['href'])\n",
    "      return extracted_links\n",
    "\n",
    "    def get_page_instance(self):\n",
    "        return {\n",
    "            'id': None,\n",
    "            'title': None,\n",
    "            'date': None,\n",
    "            '#replies': None,\n",
    "            'post': None,\n",
    "            'comments': None,\n",
    "            'related_links': None\n",
    "        }\n",
    "\n",
    "    def start_crawling(self):\n",
    "        \"\"\"\n",
    "        Start crawling the movies until the crawling threshold is reached.\n",
    "        TODO:\n",
    "            replace WHILE_LOOP_CONSTRAINTS with the proper constraints for the while loop.\n",
    "            replace NEW_URL with the new URL to crawl.\n",
    "            replace THERE_IS_NOTHING_TO_CRAWL with the condition to check if there is nothing to crawl.\n",
    "            delete help variables.\n",
    "\n",
    "        ThreadPoolExecutor is used to make the crawler faster by using multiple threads to crawl the pages.\n",
    "        You are free to use it or not. If used, not to forget safe access to the shared resources.\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "        self.extract_top_links()\n",
    "        futures = []\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            while len(self.crawled) < self.crawling_threshold:\n",
    "\n",
    "                self.add_list_lock.acquire()\n",
    "                URL = self.not_crawled.pop(0)\n",
    "                self.add_list_lock.release()\n",
    "                futures.append(executor.submit(self.crawl_page_info, URL))\n",
    "                if len(self.not_crawled) == 0:\n",
    "                    wait(futures)\n",
    "                    # print(futures[0].result())\n",
    "                    futures = []\n",
    "\n",
    "    def crawl_page_info(self, URL):\n",
    "        \"\"\"\n",
    "        Main Logic of the crawler. It crawls the page and extracts the information of the movie.\n",
    "        Use related links of a movie to crawl more movies.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        \"\"\"\n",
    "       \n",
    "\n",
    "        if self.get_id_from_URL(URL) in self.crawled_ids or len(self.crawled) >= self.crawling_threshold:\n",
    "          return\n",
    "\n",
    "        page_id = self.get_id_from_URL(URL)\n",
    "\n",
    "        res = self.crawl(URL)\n",
    "        time.sleep(10)\n",
    "\n",
    "        if (res is not None):\n",
    "            page_instance = self.get_page_instance()\n",
    "            self.extract_movie_info(res, page_instance, URL)\n",
    "\n",
    "            self.add_list_lock.acquire()\n",
    "            self.crawled.append(page_instance)\n",
    "            self.added_ids.remove(page_id)\n",
    "            self.crawled_ids.append(page_id)\n",
    "\n",
    "            for link in page_instance['related_links']:\n",
    "                self.not_crawled.append(link)\n",
    "                self.added_ids.append(self.get_id_from_URL(link))\n",
    "\n",
    "            self.add_list_lock.release()\n",
    "\n",
    "            return page_instance\n",
    "        else:\n",
    "\n",
    "            self.add_list_lock.acquire()\n",
    "            self.not_crawled.append(URL)\n",
    "            self.add_list_lock.release()\n",
    "\n",
    "\n",
    "    def extract_movie_info(self, res, page, URL):\n",
    "        \"\"\"\n",
    "        Extract the information of the movie from the response and save it in the movie instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        res: requests.models.Response\n",
    "            The response of the get request\n",
    "        movie: dict\n",
    "            The instance of the movie\n",
    "        URL: str\n",
    "            The URL of the site\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        main_soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        page['id'] = self.get_id_from_URL(URL)\n",
    "        page['title'] = self.get_title(main_soup)\n",
    "        page['date'] = self.get_date(main_soup)\n",
    "        page['#replies'] = self.get_num_replies(main_soup)\n",
    "        page['post'] = self.get_post(main_soup)\n",
    "        page['comments'] = self.get_comments(main_soup)\n",
    "        page['related_links'] = self.get_related_links(main_soup)\n",
    "\n",
    "    def get_title(self, soup):\n",
    "      title = soup.find('h1', class_='sc-20504436-1 leApID').text\n",
    "      return title\n",
    "\n",
    "    def get_date(self, soup):\n",
    "      A = soup.find_all('header', class_ = 'sc-20504436-0 KVPgp post-header')\n",
    "      a = A[0].find_all('time')\n",
    "      return a[0].text\n",
    "\n",
    "    def get_num_replies(self, soup):\n",
    "      A = soup.find_all('header', class_ = 'sc-20504436-0 KVPgp post-header')\n",
    "      try:\n",
    "        a = A[0].find_all('a')\n",
    "        ans = int(a[2].text.split(' ')[0])\n",
    "        return ans\n",
    "      except:\n",
    "        return 0\n",
    "\n",
    "    def get_post(self, soup):\n",
    "      A = soup.find_all('div', class_ = 'sc-eceb18a8-1 gdVBMy js-post-body')\n",
    "\n",
    "      ans = ''\n",
    "      for x in A:\n",
    "        a = x.find_all('p')\n",
    "        for b in a:\n",
    "          ans = ans + b.text + '\\n'\n",
    "      return ans\n",
    "\n",
    "    def get_comments(self, soup):\n",
    "      A = soup.find_all('div', class_ = 'sc-4221636f-0 bYgtgn')\n",
    "\n",
    "      ans = []\n",
    "      for x in A:\n",
    "        a = x.find_all('p')\n",
    "        txt = ''\n",
    "        for b in a:\n",
    "          txt = txt + b.text + '\\n'\n",
    "        ans.append(txt)\n",
    "      return ans\n",
    "\n",
    "    def get_related_links(self, soup):\n",
    "        \"\"\"\n",
    "        Get the related links of the movie from the More like this section of the page from the soup\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        soup: BeautifulSoup\n",
    "            The soup of the page\n",
    "        Returns\n",
    "        ----------\n",
    "        List[str]\n",
    "            The related links of the movie\n",
    "        \"\"\"\n",
    "        A = soup.find_all('div', class_ = 'sc-b58d9291-1 dNPdhv')\n",
    "        ans = []\n",
    "        for x in A:\n",
    "          a = x.find_all('a')\n",
    "          for b in a:\n",
    "            if 'posts' in b['href'] and '?responses' not in b['href']:\n",
    "              ans.append('https://healthunlocked.com' + b['href'])\n",
    "        return ans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da085b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T08:24:21.077795Z",
     "iopub.status.busy": "2025-01-04T08:24:21.077373Z",
     "iopub.status.idle": "2025-01-04T11:22:14.361080Z",
     "shell.execute_reply": "2025-01-04T11:22:14.359891Z"
    },
    "papermill": {
     "duration": 10673.28897,
     "end_time": "2025-01-04T11:22:14.363567",
     "exception": false,
     "start_time": "2025-01-04T08:24:21.074597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "crawler = Crawler(crawling_threshold=20000)\n",
    "crawler.login()\n",
    "crawler.start_crawling()\n",
    "crawler.write_to_file_as_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87bc74",
   "metadata": {
    "papermill": {
     "duration": 0.001881,
     "end_time": "2025-01-04T11:22:14.368153",
     "exception": false,
     "start_time": "2025-01-04T11:22:14.366272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10676.494815,
   "end_time": "2025-01-04T11:22:15.103600",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-04T08:24:18.608785",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
